# -*- coding: utf-8 -*-
"""ДЗ 2 Apache Spark Бобров В.С. 2 .ipynb""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OiQjHz4QIl9fK-0395Uaq8ZtG6Ka_Qwy
"""

df = spark.read.csv('/content/covid-data.csv', header=True, inferSchema=True)

df.printSchema()

df.show(5)

"""Выберите 15 стран с наибольшим процентом переболевших на 31 марта (в выходящем датасете необходимы колонки: iso_code, страна, процент переболевших)"""

from pyspark.sql.functions import col, round

df_31_march = df.filter(col("date") == "2021-03-31")

df_cases_percent = df_31_march.select(
    "iso_code",
    "location",
    "total_cases",
    "population"
).withColumn(
    "percent_infected", round((col("total_cases") / col("population")) * 100, 2)
)

top15 = df_cases_percent.orderBy(col("percent_infected").desc()).limit(15)

top15.select("iso_code", "location", "percent_infected").show(truncate=False)

top15.coalesce(1).write.csv("result_folder1", header=True, mode="overwrite")

"""Top 10 стран с максимальным зафиксированным кол-вом новых случаев за последнюю неделю марта 2021 в отсортированном порядке по убыванию
(в выходящем датасете необходимы колонки: число, страна, кол-во новых случаев)


"""

from pyspark.sql.functions import sum, round as spark_round, length

df_countries = df.filter((length("iso_code") == 3))

df_last_week = df_countries.filter((col("date") >= "2021-03-25") & (col("date") <= "2021-03-31"))

weekly_cases = df_last_week.groupBy("location").agg(
    sum("new_cases").alias("total_new_cases")
)

top10_cleaned = weekly_cases.orderBy(col("total_new_cases").desc()).limit(10)

top10_result = top10_cleaned.withColumn("кол_во_новых_случаев", spark_round("total_new_cases", 0)) \
                            .withColumnRenamed("location", "страна") \
                            .select("страна", "кол_во_новых_случаев")

top10_result.show(truncate=False)

top10_result.coalesce(1).write.csv("result_folder2", header=True, mode="overwrite")


"""Посчитайте изменение случаев относительно предыдущего дня в России за последнюю неделю марта 2021. (например: в россии вчера было 9150 , сегодня 8763, итог: -387) (в выходящем датасете необходимы колонки: число, кол-во новых случаев вчера, кол-во новых случаев сегодня, дельта)"""

from pyspark.sql.functions import col, lag, lit
from pyspark.sql.window import Window

window_spec = Window.orderBy("date")

russia_full = df.filter(col("location") == "Russia").select("date", "new_cases").orderBy("date")

russia_full = russia_full.withColumn("new_cases_yesterday", lag("new_cases").over(window_spec))

russia_full = russia_full.withColumn("delta", col("new_cases") - col("new_cases_yesterday"))

russia_week = russia_full.filter((col("date") >= "2021-03-25") & (col("date") <= "2021-03-31"))

from pyspark.sql.functions import round as spark_round

result = russia_week.select(
    col("date").alias("дата"),
    spark_round("new_cases_yesterday", 0).alias("вчера"),
    spark_round("new_cases", 0).alias("сегодня"),
    spark_round("delta", 0).alias("дельта")
)

result.show(truncate=False)

result.coalesce(1).write.csv("result_folder3", header=True, mode="overwrite")
